from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext, CallbackQueryHandler
from flask import Flask, request
import google.generativeai as genai
import matplotlib.pyplot as plt
import numpy as np
import io

import re
from dotenv import load_dotenv
import os
from generate_plot import GeneratePlot
from gemini_api import Gemini_api
from latex_generator import LatexGenerator

# Load environment variables
load_dotenv()

# Set up API keys and tokens
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
genai.configure(api_key=GEMINI_API_KEY)

# Initialize Telegram app
app = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
flask_app = Flask(__name__)

# Initialize conversation storage
user_conversations = {}
user_plot_data = {}  

# Initialize Gemini API
gemini_bot = Gemini_api()

def generate_ai_response(prompt, max_retries=3):
    """T·∫°o ph·∫£n h·ªìi t·ª´ Gemini AI v·ªõi retry"""
    for attempt in range(max_retries):
        try:
            model = genai.GenerativeModel("gemini-2.0-flash")
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            print(f"‚ùå L·ªói l·∫ßn {attempt + 1}: {e}")
            if attempt == max_retries - 1:
                return "Xin l·ªói, t√¥i ƒëang g·∫∑p v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t. Vui l√≤ng th·ª≠ l·∫°i sau."


@flask_app.route("/")
def home():
    return "Bot is running!"


@flask_app.route("/webhook", methods=["POST"])
def webhook():
    """X·ª≠ l√Ω Webhook t·ª´ Telegram"""
    update_json = request.get_json(force=True)
    print("üì© Nh·∫≠n tin nh·∫Øn t·ª´ Telegram:", update_json)
    update = Update.de_json(update_json, app)
    app.update_queue.put(update)
    return "ok"


async def start_command(update: Update, context: CallbackContext):
    """L·ªánh /start"""
    user_id = update.message.chat_id
    user_conversations[user_id] = []
    user_plot_data[user_id] = []  # Kh·ªüi t·∫°o b·ªô nh·ªõ ƒë·ªì th·ªã
    await update.message.reply_text(
        "Xin ch√†o! T√¥i l√† chatbot h·ªó tr·ª£ v·ªõi Gemini AI.")

async def handle_message(update: Update, context: CallbackContext):
    """X·ª≠ l√Ω tin nh·∫Øn"""
    user_id = update.message.chat_id
    message = update.message.text
    print(f"üì© Nh·∫≠n tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng ({user_id}): {message}")

    # Kh·ªüi t·∫°o b·ªô nh·ªõ n·∫øu ch∆∞a c√≥
    if user_id not in user_conversations:
        user_conversations[user_id] = []
    if user_id not in user_plot_data:
        user_plot_data[user_id] = []

    # X·ª≠ l√Ω y√™u c·∫ßu v·∫Ω ƒë·ªì th·ªã
    if "v·∫Ω" in message or "ƒë·ªì th·ªã" in message:
        user_conversations[user_id].append(f"Ng∆∞·ªùi d√πng: {message}")
        await GeneratePlot.generate_plot(update, context,
                            message.replace("/plot", "").strip())
        return

    # X·ª≠ l√Ω tin nh·∫Øn th√¥ng th∆∞·ªùng
    user_conversations[user_id].append(f"Ng∆∞·ªùi d√πng: {message}")
    if len(user_conversations[user_id]) > 3:
        user_conversations[user_id].pop(0)

    conversation_history = "\n".join(user_conversations[user_id])
    prompt = f"""
    Vai tr√≤ c·ªßa b·∫°n l√† m·ªôt nh√† ph√¢n t√≠ch kinh t·∫ø chuy√™n nghi·ªáp.
    - kh√¥ng tr·∫£ l·ªùi c√°c c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn kinh t·∫ø.
    - Tuy·ªát ƒë·ªëi kh√¥ng ch√†o ng∆∞·ªùi d√πng trong c√¢u n√≥i ngo·∫°i tr·ª´ ng∆∞·ªùi d√πng ch√†o b·∫°n.
    - khi ng∆∞·ªùi d√πng ch√†o th√¨ ch·ªâ gi·ªõi thi·ªáu ng·∫Øn g·ªçn kh√¥ng qu√° 4 c√¢u.
    - Ch·ªâ c·∫ßn tr·∫£ l·ªùi tr·ªçng t√¢m v√†o c√¢u h·ªèi.
    - Tr·∫£ l·ªùi l·ªãch s·ª±.
    - Tr√¨nh b√†y ƒë∆°n gi·∫£n, kh√¥ng in ƒë·∫≠m c√°c t·ª´.
    - ƒê√°nh s·ªë c√°c √Ω ch√≠nh m√† b·∫°n mu·ªën tr·∫£ l·ªùi.
    - Kh√¥ng tr·∫£ l·ªùi qu√° d√†i d√≤ng, kh√¥ng tr·∫£ l·ªùi qu√° 200 t·ª´.
    - vi·∫øt c√¢u t√≥m t·∫Øt tr∆∞·ªõc khi ph√¢n t√≠ch c√°c √Ω ch√≠nh
    ƒê√¢y l√† cu·ªôc h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥:
    {conversation_history}
    Ng∆∞·ªùi d√πng: {message}
    H√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt, logic v√† c√≥ cƒÉn c·ª© kinh t·∫ø.
    """

    response = generate_ai_response(prompt)
    response = response.replace('*', '')
    print(f"ü§ñ Ph·∫£n h·ªìi t·ª´ Gemini: {response}")

    user_conversations[user_id].append(f"Bot: {response}")

    try:
        await update.message.reply_text(response)
        print("‚úÖ G·ª≠i tin nh·∫Øn th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå L·ªói khi g·ª≠i tin nh·∫Øn: {e}")

async def report_help_command(update: Update, context: CallbackContext):
    """Show report help"""
    help_text = (
        "üîç *C√°c l·ªánh b√°o c√°o:*\n\n"
        "‚Ä¢ `/report` - B√°o c√°o kinh t·∫ø t·ªïng quan\n"
        "‚Ä¢ `/report_economic` - B√°o c√°o ph√¢n t√≠ch kinh t·∫ø\n"
        "‚Ä¢ `/report_market` - B√°o c√°o ph√¢n t√≠ch th·ªã tr∆∞·ªùng\n"
        "‚Ä¢ `/report_forecast` - B√°o c√°o d·ª± b√°o kinh t·∫ø\n"
        "‚Ä¢ `/report_custom` - B√°o c√°o kinh t·∫ø t√πy ch·ªânh\n\n"
        "Ho·∫∑c b·∫°n c√≥ th·ªÉ nh·∫≠p: 'b√°o c√°o kinh t·∫ø', 'b√°o c√°o th·ªã tr∆∞·ªùng', 'b√°o c√°o d·ª± b√°o'"
    )
    await update.message.reply_text(help_text, parse_mode="Markdown")

async def baocao_help_command(update: Update, context: CallbackContext):
    """Show report help in Vietnamese"""
    help_text = (
        "üìä *H∆∞·ªõng d·∫´n t·∫°o b√°o c√°o:*\n\n"
        "B·∫°n c√≥ th·ªÉ y√™u c·∫ßu c√°c lo·∫°i b√°o c√°o sau:\n\n"
        "‚Ä¢ 'b√°o c√°o kinh t·∫ø' - B√°o c√°o t·ªïng quan kinh t·∫ø\n"
        "‚Ä¢ 'b√°o c√°o th·ªã tr∆∞·ªùng' - Ph√¢n t√≠ch th·ªã tr∆∞·ªùng ch·ª©ng kho√°n\n"
        "‚Ä¢ 'b√°o c√°o d·ª± b√°o' - D·ª± b√°o xu h∆∞·ªõng kinh t·∫ø\n"
        "‚Ä¢ 'b√°o c√°o t√πy ch·ªânh' - B√°o c√°o theo y√™u c·∫ßu\n\n"
        "C√°ch s·ª≠ d·ª•ng: Ch·ªâ c·∫ßn nh·∫Øn tin v·ªõi n·ªôi dung 'b√°o c√°o kinh t·∫ø', 't·∫°o b√°o c√°o th·ªã tr∆∞·ªùng', v.v."
    )
    await update.message.reply_text(help_text, parse_mode="Markdown")

async def latex_command(update: Update, context: CallbackContext):
    """Handle /latex command to generate LaTeX"""
    if not context.args:
        await update.message.reply_text(
            "Vui l√≤ng cung c·∫•p m√¥ t·∫£ cho m√£ LaTeX. V√≠ d·ª•: /latex ph∆∞∆°ng tr√¨nh b·∫≠c hai"
        )
        return
    
    prompt = " ".join(context.args)
    await gemini_bot.latex_generator.generate_latex(update, context, prompt)

async def latex_list_command(update: Update, context: CallbackContext):
    """List all LaTeX files created by the user"""
    await gemini_bot.latex_generator.list_latex_files(update, context)

async def latex_get_command(update: Update, context: CallbackContext):
    """Get a specific LaTeX file"""
    await gemini_bot.latex_generator.get_latex_file(update, context)

async def latex_help_command(update: Update, context: CallbackContext):
    """Show LaTeX help"""
    help_text = (
        "üìù *H∆∞·ªõng d·∫´n t·∫°o t√†i li·ªáu PDF:*\n\n"
        "B·∫°n c√≥ th·ªÉ t·∫°o c√°c t√†i li·ªáu PDF s·ª≠ d·ª•ng LaTeX v·ªõi c√°c c√°ch sau:\n\n"
        "‚Ä¢ `/latex [m√¥ t·∫£]` - T·∫°o t√†i li·ªáu t·ª´ m√¥ t·∫£ c·ªßa b·∫°n\n"
        "‚Ä¢ Nh·∫Øn tin v·ªõi t·ª´ kh√≥a 't·∫°o pdf' ho·∫∑c 'pdf' + m√¥ t·∫£\n\n"
        "V√≠ d·ª•: \n"
        "- `/latex b√°o c√°o kinh t·∫ø v·ªõi 2 b·∫£ng v√† 1 bi·ªÉu ƒë·ªì`\n"
        "- `t·∫°o pdf ph∆∞∆°ng tr√¨nh kinh t·∫ø vƒ© m√¥`\n\n"
        "C√°c l·ªánh kh√°c:\n"
        "‚Ä¢ `/latex_list` - Xem danh s√°ch t√†i li·ªáu ƒë√£ t·∫°o\n"
        "‚Ä¢ `/latex_get [s·ªë]` - T·∫£i l·∫°i t√†i li·ªáu ƒë√£ t·∫°o\n"
        "‚Ä¢ `/latex_help` - Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n n√†y"
    )
    await update.message.reply_text(help_text, parse_mode="Markdown")

async def company_report_command(update: Update, context: CallbackContext):
    """Handle /company_report command to generate company analysis PDF"""
    if not context.args:
        await update.message.reply_text(
            "Vui l√≤ng cung c·∫•p t√™n c√¥ng ty ƒë·ªÉ ph√¢n t√≠ch. V√≠ d·ª•: /company_report Apple Inc"
        )
        return
    
    company_name = " ".join(context.args)
    prompt = f"b√°o c√°o ph√¢n t√≠ch c√¥ng ty {company_name} chi ti·∫øt, bao g·ªìm t·ªïng quan v·ªÅ doanh nghi·ªáp, ph√¢n t√≠ch t√†i ch√≠nh, SWOT, v√† d·ª± b√°o"
    await gemini_bot.latex_generator.generate_latex(update, context, prompt)

async def clear_history_command(update: Update, context: CallbackContext):
    """Clear conversation history"""
    await gemini_bot.clear_history(update, context)

def run_flask():
    """Ch·∫°y Flask ƒë·ªÉ x·ª≠ l√Ω Webhook"""
    flask_app.run(host="0.0.0.0", port=8080)

if __name__ == "__main__":
    # Initialize Gemini bot
    gemini_bot = Gemini_api()
    
    # Initialize LaTeX generator and attach it to the Gemini bot
    latex_generator = LatexGenerator(gemini_bot)
    gemini_bot.latex_generator = latex_generator

    # Add message handler
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, gemini_bot.handle_message))
    
    # Add command handlers
    app.add_handler(CommandHandler("start", start_command))
    app.add_handler(CommandHandler("latex", latex_command))
    app.add_handler(CommandHandler("latex_list", latex_list_command))
    app.add_handler(CommandHandler("latex_get", latex_get_command))
    app.add_handler(CommandHandler("latex_help", latex_help_command))
    app.add_handler(CommandHandler("company_report", company_report_command))
    app.add_handler(CommandHandler("clear_history", clear_history_command))
    
    # Add document handler to handle CSV uploads
    app.add_handler(MessageHandler(filters.Document.FileExtension("csv"), gemini_bot.handle_message))
    
    # Start the bot
    app.run_polling()
