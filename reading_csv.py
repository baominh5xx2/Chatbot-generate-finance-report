import pandas as pd
import io
import os
import matplotlib.pyplot as plt
from datetime import datetime
import re

class CSVReader:
    def __init__(self):
        self.user_csv_data = {}  # Store CSV data by user ID
        self.MAX_HISTORY = 5     # Keep only the last 5 CSV files per user
        self.temp_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), "temp_csv")
        
        # Create temp folder if it doesn't exist
        if not os.path.exists(self.temp_folder):
            os.makedirs(self.temp_folder)
    
    def process_csv(self, file_content, user_id):
        """Process CSV content and return summary information"""
        try:
            # Read CSV from bytes content
            file_stream = io.BytesIO(file_content)
            
            # Try to detect encoding and delimiter
            encoding = 'utf-8'  # Default encoding
            try:
                # Try to read with utf-8 first
                df = pd.read_csv(file_stream, encoding=encoding)
            except UnicodeDecodeError:
                # If utf-8 fails, try with latin1
                file_stream.seek(0)  # Reset file pointer
                encoding = 'latin1'
                df = pd.read_csv(file_stream, encoding=encoding)
            except pd.errors.ParserError:
                # If comma delimiter fails, try with semicolon
                file_stream.seek(0)  # Reset file pointer
                df = pd.read_csv(file_stream, encoding=encoding, sep=';')
            
            # Generate a filename with timestamp
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"user_{user_id}_{timestamp}.csv"
            filepath = os.path.join(self.temp_folder, filename)
            
            # Save the file to temp folder
            df.to_csv(filepath, index=False)
            
            # Store in user history
            if user_id not in self.user_csv_data:
                self.user_csv_data[user_id] = []
            
            # Add to user's data with metadata
            file_info = {
                "filename": filename,
                "filepath": filepath,
                "timestamp": timestamp,
                "columns": list(df.columns),
                "shape": df.shape,
                "preview": df.head(5).to_dict(),
                "dataframe": df  # Store the actual dataframe
            }
            
            self.user_csv_data[user_id].append(file_info)
            
            # Trim history if needed
            if len(self.user_csv_data[user_id]) > self.MAX_HISTORY:
                old_data = self.user_csv_data[user_id].pop(0)
                # Remove old file
                if os.path.exists(old_data["filepath"]):
                    os.remove(old_data["filepath"])
            
            # Create summary
            summary = self._generate_summary(df)
            
            return {
                "success": True,
                "filename": filename,
                "summary": summary,
                "columns": list(df.columns),
                "rows": df.shape[0],
                "dataframe": df
            }
            
        except Exception as e:
            error_msg = f"L·ªói khi x·ª≠ l√Ω file CSV: {str(e)}"
            print(error_msg)
            return {
                "success": False,
                "error": error_msg
            }
    
    def _generate_summary(self, df):
        """Generate summary statistics and information about the CSV data"""
        summary = []
        
        # Basic info
        summary.append(f"üë• S·ªë d√≤ng: {df.shape[0]}")
        summary.append(f"üìä S·ªë c·ªôt: {df.shape[1]}")
        summary.append(f"üìã T√™n c√°c c·ªôt: {', '.join(df.columns.tolist())}")
        
        # Identify numeric columns for statistics
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        if numeric_cols:
            summary.append("\nüìà Th·ªëng k√™ c√°c c·ªôt s·ªë:")
            for col in numeric_cols[:5]:  # Limit to first 5 numeric columns
                stats = df[col].describe()
                summary.append(f"  ‚Ä¢ {col}: Min={stats['min']:.2f}, Max={stats['max']:.2f}, Trung b√¨nh={stats['mean']:.2f}")
        
        # Identify potential date columns
        date_pattern = re.compile(r'date|time|ngay|thang|nam', re.IGNORECASE)
        potential_date_cols = [col for col in df.columns if date_pattern.search(col)]
        if potential_date_cols:
            summary.append("\nüìÖ C·ªôt c√≥ th·ªÉ ch·ª©a ng√†y th√°ng: " + ", ".join(potential_date_cols))
        
        # Check for missing values
        missing_data = df.isnull().sum()
        cols_with_missing = missing_data[missing_data > 0]
        if not cols_with_missing.empty:
            summary.append("\n‚ö†Ô∏è D·ªØ li·ªáu thi·∫øu:")
            for col, count in cols_with_missing.items():
                summary.append(f"  ‚Ä¢ {col}: {count} gi√° tr·ªã thi·∫øu")
        else:
            summary.append("\n‚úÖ Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu trong d·ªØ li·ªáu")
            
        return "\n".join(summary)
    
    def get_csv_preview(self, user_id):
        """Get a preview of the most recent CSV file for a user"""
        if user_id not in self.user_csv_data or not self.user_csv_data[user_id]:
            return "Kh√¥ng c√≥ d·ªØ li·ªáu CSV n√†o ƒë∆∞·ª£c l∆∞u tr·ªØ."
        
        # Get most recent file
        recent_file = self.user_csv_data[user_id][-1]
        df = recent_file["dataframe"]
        
        # Create a string representation of the first few rows
        preview = df.head(5).to_string()
        return f"Xem tr∆∞·ªõc {recent_file['filename']}:\n```\n{preview}\n```"
    
    def get_column_info(self, user_id, column_name=None):
        """Get information about a specific column or all columns"""
        if user_id not in self.user_csv_data or not self.user_csv_data[user_id]:
            return "Kh√¥ng c√≥ d·ªØ li·ªáu CSV n√†o ƒë∆∞·ª£c l∆∞u tr·ªØ."
        
        # Get most recent file
        recent_file = self.user_csv_data[user_id][-1]
        df = recent_file["dataframe"]
        
        if column_name:
            if column_name not in df.columns:
                return f"Kh√¥ng t√¨m th·∫•y c·ªôt '{column_name}' trong d·ªØ li·ªáu."
            
            # Get info about specific column
            col_data = df[column_name]
            data_type = col_data.dtype
            
            info = [f"Th√¥ng tin v·ªÅ c·ªôt '{column_name}':"]
            info.append(f"‚Ä¢ Ki·ªÉu d·ªØ li·ªáu: {data_type}")
            info.append(f"‚Ä¢ S·ªë gi√° tr·ªã duy nh·∫•t: {col_data.nunique()}")
            
            # For numeric columns
            if pd.api.types.is_numeric_dtype(col_data):
                stats = col_data.describe()
                info.append(f"‚Ä¢ Gi√° tr·ªã nh·ªè nh·∫•t: {stats['min']}")
                info.append(f"‚Ä¢ Gi√° tr·ªã l·ªõn nh·∫•t: {stats['max']}")
                info.append(f"‚Ä¢ Gi√° tr·ªã trung b√¨nh: {stats['mean']:.2f}")
                info.append(f"‚Ä¢ ƒê·ªô l·ªách chu·∫©n: {stats['std']:.2f}")
            
            # For string/object columns
            elif pd.api.types.is_string_dtype(col_data) or pd.api.types.is_object_dtype(col_data):
                # Get most common values
                most_common = col_data.value_counts().head(3)
                if not most_common.empty:
                    info.append("‚Ä¢ C√°c gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t:")
                    for value, count in most_common.items():
                        info.append(f"  - {value}: {count} l·∫ßn")
            
            # Check for missing values
            missing = col_data.isnull().sum()
            if missing > 0:
                info.append(f"‚Ä¢ S·ªë gi√° tr·ªã thi·∫øu: {missing} ({missing/len(col_data)*100:.1f}%)")
            else:
                info.append("‚Ä¢ Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu")
                
            return "\n".join(info)
        else:
            # Get quick summary of all columns
            info = ["Th√¥ng tin t·∫•t c·∫£ c√°c c·ªôt:"]
            for col in df.columns:
                data_type = df[col].dtype
                missing = df[col].isnull().sum()
                unique = df[col].nunique()
                info.append(f"‚Ä¢ {col}: {data_type}, {unique} gi√° tr·ªã duy nh·∫•t, {missing} gi√° tr·ªã thi·∫øu")
            
            return "\n".join(info)
    
    def generate_quick_plot(self, user_id, plot_description):
        """Generate a quick plot based on the latest CSV data"""
        if user_id not in self.user_csv_data or not self.user_csv_data[user_id]:
            return None, "Kh√¥ng c√≥ d·ªØ li·ªáu CSV n√†o ƒë∆∞·ª£c l∆∞u tr·ªØ."
        
        # Get most recent file
        recent_file = self.user_csv_data[user_id][-1]
        df = recent_file["dataframe"]
        
        try:
            plt.figure(figsize=(10, 6))
            
            # Check what kind of plot is requested
            plot_description = plot_description.lower()
            
            # Find column names mentioned in the description
            mentioned_cols = [col for col in df.columns if col.lower() in plot_description]
            
            # If no specific columns are mentioned, use the first numeric columns
            numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
            
            if not mentioned_cols and numeric_cols:
                if "pie" in plot_description or "tr√≤n" in plot_description:
                    # For pie charts, use the first categorical column and its counts
                    cat_cols = df.select_dtypes(include=['object']).columns.tolist()
                    if cat_cols:
                        counts = df[cat_cols[0]].value_counts()
                        counts.plot(kind='pie', autopct='%1.1f%%')
                        plt.title(f"Bi·ªÉu ƒë·ªì tr√≤n {cat_cols[0]}")
                    else:
                        # If no categorical columns, use the first numeric column binned into categories
                        df[numeric_cols[0]].plot(kind='hist', bins=5)
                        plt.title(f"Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi {numeric_cols[0]}")
                
                elif "bar" in plot_description or "c·ªôt" in plot_description:
                    # For bar charts, use the first two columns if possible
                    if len(numeric_cols) >= 2:
                        df[numeric_cols[0]][:20].plot(kind='bar')
                        plt.title(f"Bi·ªÉu ƒë·ªì c·ªôt {numeric_cols[0]}")
                    else:
                        df[numeric_cols[0]][:20].plot(kind='bar')
                        plt.title(f"Bi·ªÉu ƒë·ªì c·ªôt {numeric_cols[0]}")
                
                elif "scatter" in plot_description or "ƒëi·ªÉm" in plot_description:
                    # For scatter plots, use first two numeric columns
                    if len(numeric_cols) >= 2:
                        plt.scatter(df[numeric_cols[0]], df[numeric_cols[1]])
                        plt.xlabel(numeric_cols[0])
                        plt.ylabel(numeric_cols[1])
                        plt.title(f"Bi·ªÉu ƒë·ªì ƒëi·ªÉm {numeric_cols[0]} vs {numeric_cols[1]}")
                    else:
                        plt.scatter(range(len(df)), df[numeric_cols[0]])
                        plt.xlabel("Ch·ªâ m·ª•c")
                        plt.ylabel(numeric_cols[0])
                        plt.title(f"Bi·ªÉu ƒë·ªì ƒëi·ªÉm {numeric_cols[0]}")
                
                else:  # Default to line chart
                    df[numeric_cols[0]][:50].plot(kind='line')
                    plt.title(f"Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng {numeric_cols[0]}")
            
            else:  # Use mentioned columns
                col = mentioned_cols[0]  # Use the first mentioned column
                
                if pd.api.types.is_numeric_dtype(df[col]):
                    if "pie" in plot_description or "tr√≤n" in plot_description:
                        # For numeric column in pie chart, we need to bin it
                        df[col].value_counts(bins=5).plot(kind='pie', autopct='%1.1f%%')
                        plt.title(f"Bi·ªÉu ƒë·ªì tr√≤n {col} (ƒë√£ nh√≥m)")
                    
                    elif "bar" in plot_description or "c·ªôt" in plot_description:
                        df[col][:20].plot(kind='bar')
                        plt.title(f"Bi·ªÉu ƒë·ªì c·ªôt {col}")
                    
                    elif "hist" in plot_description or "ph√¢n ph·ªëi" in plot_description:
                        df[col].plot(kind='hist', bins=10)
                        plt.title(f"Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi {col}")
                    
                    else:  # Default to line chart
                        df[col][:50].plot(kind='line')
                        plt.title(f"Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng {col}")
                
                else:  # Categorical column
                    counts = df[col].value_counts()
                    if "pie" in plot_description or "tr√≤n" in plot_description:
                        counts.plot(kind='pie', autopct='%1.1f%%')
                        plt.title(f"Bi·ªÉu ƒë·ªì tr√≤n {col}")
                    else:
                        counts.plot(kind='bar')
                        plt.title(f"Bi·ªÉu ƒë·ªì c·ªôt {col}")
            
            # Save to buffer
            buffer = io.BytesIO()
            plt.tight_layout()
            plt.savefig(buffer, format='png', dpi=300)
            buffer.seek(0)
            plt.close()
            
            return buffer, "Th√†nh c√¥ng"
            
        except Exception as e:
            plt.close()
            error_msg = f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì: {str(e)}"
            print(error_msg)
            return None, error_msg

    def prepare_csv_analysis_data(self, user_id):
        """Prepare CSV data for PDF analysis"""
        if user_id not in self.user_csv_data or not self.user_csv_data[user_id]:
            return None, "Kh√¥ng t√¨m th·∫•y file CSV n√†o. Vui l√≤ng t·∫£i l√™n tr∆∞·ªõc khi y√™u c·∫ßu ph√¢n t√≠ch."
        
        # Get the most recent CSV file
        recent_file = self.user_csv_data[user_id][-1]
        df = recent_file["dataframe"]
        
        # Generate basic statistics and insights
        analysis_data = {
            "filename": recent_file["filename"],
            "rows": df.shape[0],
            "columns": df.shape[1],
            "column_names": list(df.columns),
            "summary": self._generate_summary(df),
            "head": df.head(10).to_html(),
            "describe": df.describe().to_html(),
            "missing_values": df.isnull().sum().to_dict(),
            "column_types": {col: str(df[col].dtype) for col in df.columns},
        }
        
        # Generate additional insights based on data types
        insights = []
        
        # Analyze numeric columns
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        if numeric_cols:
            insights.append("### Ph√¢n t√≠ch c·ªôt s·ªë:")
            for col in numeric_cols[:5]:  # Limit to first 5 numeric columns for brevity
                col_insights = []
                # Basic statistics
                stats = df[col].describe()
                col_insights.append(f"- **{col}**: Min={stats['min']:.2f}, Max={stats['max']:.2f}, Mean={stats['mean']:.2f}, Std={stats['std']:.2f}")
                
                # Check for potential outliers using IQR method
                Q1 = stats['25%']
                Q3 = stats['75%']
                IQR = Q3 - Q1
                outlier_count = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()
                if outlier_count > 0:
                    col_insights.append(f"  - C√≥ {outlier_count} gi√° tr·ªã c√≥ th·ªÉ l√† ngo·∫°i lai (~{outlier_count/len(df)*100:.1f}%)")
                
                # Check distribution
                skew = df[col].skew()
                if abs(skew) > 1:
                    skew_direction = "ph·∫£i" if skew > 0 else "tr√°i"
                    col_insights.append(f"  - Ph√¢n ph·ªëi l·ªách {skew_direction} (skewness={skew:.2f})")
                
                insights.extend(col_insights)
        
        # Analyze categorical columns
        cat_cols = df.select_dtypes(include=['object']).columns.tolist()
        if cat_cols:
            insights.append("\n### Ph√¢n t√≠ch c·ªôt ph√¢n lo·∫°i:")
            for col in cat_cols[:5]:  # Limit to first 5 categorical columns
                value_counts = df[col].value_counts()
                top_categories = value_counts.head(3).to_dict()
                unique_count = df[col].nunique()
                insights.append(f"- **{col}**: {unique_count} gi√° tr·ªã duy nh·∫•t")
                
                # Common categories
                categories_str = ", ".join([f"{k} ({v})" for k, v in top_categories.items()])
                insights.append(f"  - C√°c gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t: {categories_str}")
                
                # Check for high cardinality
                if unique_count > df.shape[0] * 0.5 and unique_count > 10:
                    insights.append(f"  - C·ªôt n√†y c√≥ ƒë·ªô ph√¢n t√°n cao, c√≥ th·ªÉ l√† ID ho·∫∑c d·ªØ li·ªáu duy nh·∫•t")
        
        # Look for potential date columns
        date_pattern = re.compile(r'date|time|ngay|thang|nam', re.IGNORECASE)
        potential_date_cols = [col for col in df.columns if date_pattern.search(col)]
        if potential_date_cols:
            insights.append("\n### Ph√¢n t√≠ch c·ªôt th·ªùi gian:")
            for col in potential_date_cols:
                insights.append(f"- **{col}** c√≥ th·ªÉ ch·ª©a th√¥ng tin th·ªùi gian")
        
        # Add insights about missing values
        missing_data = df.isnull().sum()
        cols_with_missing = missing_data[missing_data > 0]
        if not cols_with_missing.empty:
            insights.append("\n### Ph√¢n t√≠ch d·ªØ li·ªáu thi·∫øu:")
            for col, count in cols_with_missing.items():
                pct = count / len(df) * 100
                insights.append(f"- **{col}**: {count} gi√° tr·ªã thi·∫øu ({pct:.2f}%)")
                if pct > 20:
                    insights.append(f"  - C·ªôt n√†y c√≥ t·ª∑ l·ªá thi·∫øu cao, c·∫ßn x·ª≠ l√Ω th·∫≠n tr·ªçng")
        
        # Add insights to the analysis data
        analysis_data["insights"] = "\n".join(insights)
        
        return analysis_data, "success"
